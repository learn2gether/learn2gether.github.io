---
title: "State Final Demand Forecasting Part I"
date: 2019-09-01
tags: [finance analysis, data science, forecasting, VAR]
# header:
#     image: "images/posts/titanic.jpeg"
excerpt: "A Case Study on forecasting of state final demand by VAR model."
---

![alt text](https://learn2gether.github.io/images/posts/sfd/sfd.jpg "SFD")

<br />

- [Introduction](#introduction)
- [SDF Data Exploration](#sdf-data-exploration)
- [Model Selection](#model-selection)
  * [ARIMA Model](#arima-model)
  * [VAR Model](#var-model)
- [VAR Model Exploration](#var-model-exploration)
  * [Data Exploration](#data-exploration)
  * [Test For Stationarity](#test-for-stationarity)
  * [Model Building](#model-building)
  * [Test For Serial Correlation Of Residuals](#test-for-serial-correlation-of-residuals)
  * [Impulse Response Analysis](#impulse-response-analysis)
  * [Forecast Error Variance Decomposition](#forecast-error-variance-decomposition)
  * [Test For Causation](#test-for-causation)
  * [Cointegration](#cointegration)
  * [Forecasting](#forecasting)

<br />

# Introduction

<div style="text-align: justify"> Gross domestic product (GDP) is a measurement of demand for products reflecting economic status in a country, and gross state product (GSP) represents the state economy. We can consider GSP as a representation measured by a sample data, which could also reflect the national economy. Thus, GDP and GSP usually are positively correlated. State final demand (SFD) is another measurement of the economy for state, and it has four components including household consumption, government consumption, private capital formation and public capital formation. SFD is different from GSP as GSP has two more components which are changes in inventories and export and import trading. Ultimately, SFD for each state is going to affect Australian domestic final demand. Household final consumption expenditure is driven by the consumption of electricity, fuel, food, clothing, recreation and so on. The private capital formation is driven by private investment such as the housing market, and the public investment is driven by the state and local governments. In short, final state demand is an indicator of economic activities reflecting the total consumption of an economy in goods and services that do not include international and interstate trade. SFD is important for managing the state economy as it measures whether each state can maintain a balance between aggregate supply and demand. The demand in economic growth usually has a significant increase, and it reflects the fluctuation on the economy. </div>
<br />

# SDF Data Exploration

<div style="text-align: justify"> There are three types of SFD data series including original data, trend estimates and seasonal adjustment. Original data (Abs.gov.au, 2012) reflects all patterns of time series, and it is suitable for analysis and forecasting based on historical data. Seasonal adjustment (Abs.gov.au, 2012) is a process of removing seasonal influences from the original time series, and the reason is to reveal the characteristics of quarterly or monthly sequences. There are some drawbacks of seasonal adjustment. At first, the adjusted time series are the result of the analysis, and we may not observe directly. Also, the original time series are independent of each other. The seasonal characteristics of adjusted time series change becoming interrelated. Trend adjustment (Abs.gov.au, 2012) is a process of smoothing noise from seasonally adjusted sequences. Thus, trend estimates have similar drawbacks with seasonal adjustment. In short, we are going to use original estimates for forecasting in this article. </div>

<br />

<div style="text-align: justify"> The original estimates are measured by chain volume measures. Chain volume measures (Aph.gov.au, n.d.) is based on the weight of the price index of the previous financial year. Chain volume measures can reflect the current economic status by rebasing price index every year, thereby providing better estimates, especially in times of accelerated product upgrading. For example, high-tech products like computers, electronic products and mobile phones have a relatively short period of products iteration. Thus, the price of these products changes rapidly. </div>

<br />

# Model Selection
## ARIMA Model

<div style="text-align: justify"> ARIMA model consists of autoregressive model, integration and moving average model. Autoregressive model only depends on its previous lagged value without any other explanatory variables. Integration meaning that the time series is differentiated through the model. Time series analysis requires stationary sequences, and we need to convert the unstable sequence into a stationary sequence by differential. As for the moving average model, and the value of a certain time point is affected by the prediction error in the past. The prediction error is the difference between the prediction and the true value. In short, ARIMA model only requires an endogenous variable without other exogenous variables. In our case, the input variable is only SFD itself. The SFD normally grows steadily over time, so its time series is non-stationary. </div>

<br />

## VAR Model

<div style="text-align: justify"> VAR represents the vector  utoregression model, and it is applied to forecast time series which are interconnected to each other. The VAR model is built by considering all other endogenous variables as the hysteresis of one variable. Thus, VAR model is a multivariate autoregressive model, and it consists of multiple time series. If multiple non-stationary time series have a linear combination, this combination may be stationary. Also, if such a linear combination exists, these time series are cointegrated, which is able to interpret a long-term equilibrium relationship between these factors. In our case, the SFD consist of four components, and the input can be multiple time series. </div>

<br />

# VAR Model Exploration
## Data Exploration

<div style="text-align: justify"> The following Figure shows the WA original data from the September of 1985 to the March of 2019. </div>

<br />

![alt text](https://learn2gether.github.io/images/posts/sfd/sfd_wa_1985_2019.png "SFD WA")

<br />

<div style="text-align: justify"> The data is measured quarterly, and there are 135 observations with five variables. The unit of each variable is millions. According to the following figure, we can visualize each time
series sequence after we make sure the dataset is clean without any missing values. </div>

<br />

```python
# Plot
fig, axes = plt.subplots(nrows=3, ncols=2, dpi=120, figsize=(10,6))
for i, ax in enumerate(axes.flatten()):
    if i<5:
        data = wa[wa.columns[i]]
        ax.plot(data, color='red', linewidth=1)
        # Decorations
        ax.set_title(wa.columns[i])
    ax.xaxis.set_ticks_position('none')
    ax.yaxis.set_ticks_position('none')
    ax.spines["top"].set_alpha(0)
    ax.tick_params(labelsize=6)
# fig.delaxes(axes[3,2])
fig.delaxes(axes.flat[-1])
plt.tight_layout()
```

<br />

![alt text](https://learn2gether.github.io/images/posts/sfd/wa_sfd.png "WA SFD")

<br />

<div style="text-align: justify"> All time series sequences have the increasing trend patterns over time, even though the public capital formation has a dramatic change in the March of 1998. </div>

<br />

## Test For Stationarity

<div style="text-align: justify"> First, we should test whether each time series is stationary as the VAR model requires all input variables stationary for the forecasting. The stationarity means that the sequence has constant characteristics such as variance and mean. We can use Augmented Dickey-Fuller test (ADF Test) which is one of unit-root tests. According to the following figure, none of them is stationary since all p-value is greater than 0.05, accepting null hypothesis. The null hypothesis is that all time series are non-stationary. Thus, we need to difference them till all of them are stationary. </div>

<br />

```python
# ADF test
def adfuller_test(series, signif=0.05, name='', verbose=False):
    """Perform ADFuller to test for Stationarity of given series and print report"""
    r = adfuller(series, autolag='AIC')
    output = {'test_statistic':round(r[0], 4), 'pvalue':round(r[1], 4), 'n_lags':round(r[2], 4), 'n_obs':r[3]}
    p_value = output['pvalue'] 
    def adjust(val, length= 6): return str(val).ljust(length)

    # Print Summary
    print(f'    Augmented Dickey-Fuller Test on "{name}"', "\n   ", '-'*47)

    if p_value <= signif:
        print(f" => P-Value = {p_value}. Rejecting Null Hypothesis.")
        print(f" => P-Value = {p_value}.")
        #print(f" => Series is Stationary.")
    else:
        print(f" => P-Value = {p_value}. Weak evidence to reject the Null Hypothesis.")
        print(f" => P-Value = {p_value}.")
        #print(f" => Series is Non-Stationary.")    
```

```python
# ADF Test on each column
print('Western Australia')
for name, column in wa_train.iteritems():
    adfuller_test(column, name=column.name)
```

<br />

![alt text](https://learn2gether.github.io/images/posts/sfd/adf_test.png "ADF Test")

<br />

```python
# 1st difference
# diff1_wa = wa_train.diff().dropna()
diff1_wa = wa_train.diff().dropna()
```

<div style="text-align: justify"> The following figure shows that only the public gross fixed capital formation is stationary after the first differencing, so we need to do the second differencing. </div>

<br />

![alt text](https://learn2gether.github.io/images/posts/sfd/first_diff.png "The First Difference")

<br />

<div style="text-align: justify"> The second differencing makes all sequences stationary as all P-values are less than 0.05, which rejects the null hypothesis. </div>

<br />

![alt text](https://learn2gether.github.io/images/posts/sfd/second_diff.png "The Second Difference")

<br />

<div style="text-align: justify"> According to the following figure, we can also visually examine that all sequences are stationary after the second differencing. </div>

<br />

![alt text](https://learn2gether.github.io/images/posts/sfd/stationary.png "Stationary")

<br />

## Model Building

<div style="text-align: justify"> First, we split dataset into training set and testing set. Then the training set is used to fit the VAR model. Later on we will use the testing set to forecast. The testing set consist of recent four quarters from the June of 2018 to the March of 2019.</div>

<br />

<div style="text-align: justify"> In order to build VAR model, we need to select the right lag order first. We can fit VAR model by different orders, then select the lag order by comparing the AIC and BIC. According to the following figure, we can find out that the VAR model has the least AIC and BIC at lag 3, so we select the lag 3 model. </div>

<br />

![alt text](https://learn2gether.github.io/images/posts/sfd/lag.png "Lag")

<br />

```python
print('WA ********************************')
# lag 3 for both AIC and BIC
wa_model = VAR(diff2_wa)
for i in [1,2,3,4,5,6,7,8,9]:
    result = wa_model.fit(i)
    print('Lag Order =', i)
    print('AIC : ', result.aic)
    print('BIC : ', result.bic)
```

<div style="text-align: justify"> The following figure shows the output after fitting the model. Therefore, the following equation shows the model. </div>

<br />

![alt text](https://learn2gether.github.io/images/posts/sfd/var_equation.png "VAR Equation")

![alt text](https://learn2gether.github.io/images/posts/sfd/var_result.png "VAR Result")

<br />

## Test For Serial Correlation Of Residuals

<div style="text-align: justify"> We run this test to check whether there is any pattern left by residuals. If we cannot pass this test, it means that our model cannot explain some patterns in the time series. There are some tests such as Durbin Watson’ Statistic and Ljung-Box Q test. The following figure shows the result of Durbin Watson Statistic. The range of this statistic is from 0 to 4. If the value is closer to 2, it represents that there is no significant auto-correlation. If the value is closer to 2 or 4, it represents that there is a positive or negative serial correlation respectively. From the output, we can see that our model can explain the variances in the time series. </div>

<br />

![alt text](https://learn2gether.github.io/images/posts/sfd/durbin_watson.png "Durbin Watson Statistic")

<br />

<div style="text-align: justify"> According to the plot of residual autocorrelation, each of time series is moving around the significant level with max lag order of 10. Thus, there is no obvious autocorrelation between the signal and its lagged version. </div>

<br />

```python
wa_model_fitted.plot_acorr(nlags=10, resid=True, linewidth=6)
plt.show()
```

![alt text](https://learn2gether.github.io/images/posts/sfd/residual_autocorrelation.png "Residual Autocorrelation")

<br />

<div style="text-align: justify"> The following figure shows the result of Ljung-Box Q test, and P-values of SFD are all greater than 0.05 in different lag order. Thus, there is a weak evidence to reject the null hypothesis, and the residual is white noise. </div>

<br />

```python
from statsmodels.tsa.stattools import acf
(resid_acf, qstat, pvalue) = acf(wa_model_fitted.resid['SFD'], nlags=10, qstat=True)
print('STATE FINAL DEMAND')
print('lag               Q-stat               P-Value')
for i in range(0,len(qstat)):
    print(i+1, '       ',qstat[i],'      ',pvalue[i])
```

![alt text](https://learn2gether.github.io/images/posts/sfd/ljung_box_q.png "Ljung Box Q Test")

<br />

## Impulse Response Analysis

<div style="text-align: justify"> In the VAR model, when a variable is affected by an exogenous shock, there are usually dynamic effects to other variables. The following figure shows the impact on SFD by government consumption, household consumption, private and public capital formations from left to right respectively. The impact on SFD by each of them is not fixed, which can be positive and negative impact at different period. The government consumption has a slightly negative impact on SFD at first 3 quarters. Then that impact reaches the maximum at 4th quarter. After that, that impact decreases and increases back and forth. The household’s consumption has the similar pattern with government consumption. As for private and public capital formations, they also share same pattern. Their impact continues to around 8th quarter. </div>

<br />

![alt text](https://learn2gether.github.io/images/posts/sfd/impact_on_sfd.png "Impact On SFD")

<br />

## Forecast Error Variance Decomposition

<div style="text-align: justify"> FEVD is able to further evaluate how important of each structural shock by analyzing their contribution to SFD. According to the following figure, we cannot tell that four components have impact on SFD reaching stable till 10th quarter. The next figure shows that private capital formation is the most significant factor to SFD followed by public capital formation, household consumption, government consumption. </div>

<br />

![alt text](https://learn2gether.github.io/images/posts/sfd/FEVD1.png "FEVD")

<br />

![alt text](https://learn2gether.github.io/images/posts/sfd/FEVD2.png "FEVD")

<br />

## Test For Causation

<div style="text-align: justify"> The basis behind VAR model is that each sequence in the system affects each other, so we can make the prediction by using historical data of itself along with other sequences. In order to test causation, we can use Granger’s Causality Test. If original time series are not stationary, we should difference them into stationary sequences as inputs of Granger’s Causality Test. The null hypothesis is that time series in the system do not influence each other. If the p-value is less than 0.05, there is a strong evidence to reject the null hypothesis. Also, we usually use the Granger’s Causality Test to find out which sequence is exogenous variable once we know the lag order which is 3 in our case. According to the output by the following figure, all P-values are less than the significant level of 0.05, meaning that each of time series affects each other, and all of them are endogenous variables. Each row represents a response variable (Y), and each column represents an independent variable (X). For example, P-value of 0.0169 in row 1, column 4, representing that the public capital formation causes government consumption as it is below the significant level of 0.05. This result makes our data be perfect to apply VAR model for prediction. </div>

<br />

```python
from statsmodels.tsa.stattools import grangercausalitytests
maxlag=3
test = 'ssr_chi2test'
def grangers_causation_matrix(data, variables, test='ssr_chi2test', verbose=False):    
    df = pd.DataFrame(np.zeros((len(variables), len(variables))), columns=variables, index=variables)
    for c in df.columns:
        for r in df.index:
            test_result = grangercausalitytests(data[[r, c]], maxlag=maxlag, verbose=False)
            p_values = [round(test_result[i+1][0][test][1],4) for i in range(maxlag)]
            if verbose: print(f'Y = {r}, X = {c}, P Values = {p_values}')
            min_p_value = np.min(p_values)
            df.loc[r, c] = min_p_value
    df.columns = [var + '_x' for var in variables]
    df.index = [var + '_y' for var in variables]
    return df

grangers_causation_matrix(diff2_wa, variables = wa_train.columns) 
```

<br />

![alt text](https://learn2gether.github.io/images/posts/sfd/grangers_causality.png "Grangers Causality Test")

<br />

## Cointegration

<div style="text-align: justify"> We can run cointegration test to determine whether there is a stable linear relationship in the long run among multiple time series which are unstable. By running cointegration test, we should use the original data rather than their differenced data. At 5% level, there is a strong evidence to reject the null hypothesis if the statistics value of the Trace and Max is greater than 5% critical value. Otherwise, there is a weak evidence to reject the null hypothesis. The following figure shows the results of Johansen test. Both test at significant level of 5% agreed that there is at most 2 cointegrating vectors. </div>

<br />

```python
result_vecm = coint_johansen(wa_train,det_order=-1, k_ar_diff=3)
trace_result = np.c_[result_vecm.lr1, result_vecm.cvt] 
max_result = np.c_[result_vecm.lr2, result_vecm.cvm]
index = np.arange(0,5)
col = ['Test statistics', 'CV at 10%', 'CV at 5%', 'CV at 1%']
trace = pd.DataFrame(trace_result, index=index, columns=col)
maxt = pd.DataFrame(trace_result, index=index, columns=col)
trace, maxt
```

<br />

![alt text](https://learn2gether.github.io/images/posts/sfd/johansen_test.png "Johansen Test")

<br />

## Forecasting

<div style="text-align: justify"> According to the previous process, the forecasting requires the lag order of 3. Thus, we need to extract data of recent three quarter from the differenced training set. </div>

<br />

![alt text](https://learn2gether.github.io/images/posts/sfd/forecasts_diff.png "Forecasts Diff")

<br />

<div style="text-align: justify"> The figure shows the prediction of testing set. However, the forecasts are generated by the model using the differenced training set, and we need to de-difference them by the number of differencing which we did on the original data. </div>

<br />

![alt text](https://learn2gether.github.io/images/posts/sfd/forecast_original.png "Original Forecasts")

<br />

<div style="text-align: justify"> In order to compare the forecasts with actual values, we plot them in the same graph. According to the figure, even though forecasts are different compared to the actual values, they have similar pattern, which can help us identify the trend. In this figure, we only need to look at the last subplot since the model is built to predict SFD. </div>

<br />

![alt text](https://learn2gether.github.io/images/posts/sfd/forecast_plot.png "Forecasts Plot")

<br />

<div style="text-align: justify"> I am going to discuss the implementation of the ARIMA model and the comparison between those two models in the next article.</div>


