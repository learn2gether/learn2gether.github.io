---
title: "BMI Analysis"
date: 2018-03-01
tags: [machine learning, data science]
# header:
#     image: "images/posts/titanic.jpeg"
excerpt: "A Case Study on Body Mass Index."
---

![alt text](https://learn2gether.github.io/images/posts/bmi/bmi.png "BMI")
<br />

- [Introduction](#introduction)
- [Data Exploration](#data-exploration)
- [Handling Missing Values](#handling-missing-values)
- [Handling Outliers](#handling-outliers)
- [Exploratory Data Analysis](#exploratory-data-analysis)
  * [Height Weight and BMI](#height-weight-and-bmi)
  * [Gender](#gender)
  * [Geographic Analysis](#geographic-analysis)
- [Modelling](#modelling)
  * [Ordered Logit Model](#ordered-logit-model)
  * [Ordered Probit Model](#ordered-probit-model)
  * [Multinomial Logit Model](#multinomial-logit-model)
- [Model Selection](#model-selection)
  * [Likelihood Ratio Test](#likelihood-ratio-test)
  * [Akaike Information Criterion](#akaike-information-criterion)
- [Limitation](#limitation)
- [Suggestions On Monetary Implications](#suggestions-on-monetary-implications)

<br />

# Introduction
<div style="text-align: justify"> Body Mass Index (BMI) has become a significant measurement to distinguish our health condition. It is usually categorized as four levels including underweight, normal, overweight and obese. If people are categorized as the level of obese, they have high risk to have several diseases such as heart disease, high blood pressure and diabetes. The government has paid more attention on the physical condition of residents since it directly affects the government financial budget. The objective of this article is to conduct an exploration analysis and then build a proper model for the government based on a health survey. This report could help the government have a certain understanding of how each feature influences people’s health. Later on, the government is able to develop some strategies on budge distribution. This dataset consists of 7500 observations with 237 features. The analysis is based on Python, R and a few supported libraries. Python has an excellent performance on data visualization to generate more interactive plots. Since we are mainly dealing with statistical models, R is the best choice to conduct statistical analysis and generate statistical results. </div>
<br />

```python
df, meta = pyreadstat.read_sav("BMI_Survey_Victoria.sav")
```

```python
df.head()
```
<br />
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>id</th>
      <th>jb_</th>
      <th>ij_</th>
      <th>jc_</th>
      <th>ik_</th>
      <th>bs_</th>
      <th>bs_2</th>
      <th>bu_</th>
      <th>sex</th>
      <th>eb_a</th>
      <th>eb_b</th>
      <th>eb_c</th>
      <th>eb_d</th>
      <th>eb_e</th>
      <th>eb_f</th>
      <th>eb_g</th>
      <th>eo_</th>
      <th>ep_a</th>
      <th>ep_b</th>
      <th>ep_c</th>
      <th>ep_d</th>
      <th>ep_e</th>
      <th>ep_f</th>
      <th>ep_g</th>
      <th>ep_h</th>
      <th>ep_i</th>
      <th>ep_j</th>
      <th>ep_k</th>
      <th>ep_m</th>
      <th>en_a</th>
      <th>en_b</th>
      <th>en_c</th>
      <th>en_d</th>
      <th>en_e</th>
      <th>mn_</th>
      <th>mw_</th>
      <th>gx_</th>
      <th>gy_</th>
      <th>db_</th>
      <th>nu_</th>
      <th>nv_</th>
      <th>jl_</th>
      <th>jm_</th>
      <th>jn_</th>
      <th>jr_</th>
      <th>js_</th>
      <th>jt_</th>
      <th>ju_</th>
      <th>hb_</th>
      <th>hd_</th>
      <th>he_</th>
      <th>oe_</th>
      <th>hf_</th>
      <th>hg_</th>
      <th>oj_</th>
      <th>le_</th>
      <th>ok_</th>
      <th>ol_</th>
      <th>om_</th>
      <th>cx_</th>
      <th>da_</th>
      <th>dl_</th>
      <th>dm_</th>
      <th>dn_a</th>
      <th>dn_b</th>
      <th>dn_c</th>
      <th>dn_i</th>
      <th>di_</th>
      <th>mk_a</th>
      <th>mk_b</th>
      <th>mk_c</th>
      <th>mk_d</th>
      <th>mk_e</th>
      <th>mk_f</th>
      <th>mk_g</th>
      <th>mk_h</th>
      <th>dk_</th>
      <th>dr_</th>
      <th>dq_</th>
      <th>ds_</th>
      <th>dt_</th>
      <th>dv_</th>
      <th>du_a</th>
      <th>du_b</th>
      <th>du_c</th>
      <th>du_d</th>
      <th>du_e</th>
      <th>du_f</th>
      <th>du_g</th>
      <th>dw_</th>
      <th>dx_</th>
      <th>hm_</th>
      <th>jy_</th>
      <th>jz_</th>
      <th>ho_</th>
      <th>ke_</th>
      <th>kf_</th>
      <th>hq_</th>
      <th>kk_</th>
      <th>kl_</th>
      <th>cc_</th>
      <th>ce_a</th>
      <th>ce_b</th>
      <th>ce_c</th>
      <th>ce_d</th>
      <th>ce_e</th>
      <th>ce_f</th>
      <th>ce_g</th>
      <th>ce_h</th>
      <th>ce_i</th>
      <th>ce_j</th>
      <th>co_a</th>
      <th>co_b</th>
      <th>co_c</th>
      <th>co_d</th>
      <th>co_e</th>
      <th>co_f</th>
      <th>hz_a</th>
      <th>hz_b</th>
      <th>hz_c</th>
      <th>hz_d</th>
      <th>hz_e</th>
      <th>hz_f</th>
      <th>hz_g</th>
      <th>hz_i</th>
      <th>hw_a</th>
      <th>hw_b</th>
      <th>hw_c</th>
      <th>hw_d</th>
      <th>hw_e</th>
      <th>hx_</th>
      <th>hy_</th>
      <th>pd_</th>
      <th>pe_</th>
      <th>pf_</th>
      <th>pg_</th>
      <th>fw_</th>
      <th>ph_</th>
      <th>fp_</th>
      <th>fq_</th>
      <th>pi_</th>
      <th>pj_</th>
      <th>pk_</th>
      <th>gc_</th>
      <th>gj_</th>
      <th>pm_a</th>
      <th>pm_b</th>
      <th>pm_c</th>
      <th>pm_d</th>
      <th>pm_f</th>
      <th>li_a</th>
      <th>li_b</th>
      <th>li_c</th>
      <th>li_d</th>
      <th>li_e</th>
      <th>li_f</th>
      <th>li_cu</th>
      <th>po_</th>
      <th>fr_</th>
      <th>pp_</th>
      <th>jf_</th>
      <th>gf_</th>
      <th>gg_</th>
      <th>gi_</th>
      <th>gh_</th>
      <th>gk_</th>
      <th>gd_</th>
      <th>jh_</th>
      <th>gm_</th>
      <th>ig_</th>
      <th>ml_</th>
      <th>ii_</th>
      <th>ih_</th>
      <th>mz_</th>
      <th>je_</th>
      <th>in_</th>
      <th>ip_</th>
      <th>io_</th>
      <th>lc_</th>
      <th>ld_</th>
      <th>iq_a</th>
      <th>iq_b</th>
      <th>iq_c</th>
      <th>iq_d</th>
      <th>iq_e</th>
      <th>iq_f</th>
      <th>iq_g</th>
      <th>iq_h</th>
      <th>iq_i</th>
      <th>iq_j</th>
      <th>iq_k</th>
      <th>it_</th>
      <th>iu_</th>
      <th>lb_</th>
      <th>mm_</th>
      <th>iv_</th>
      <th>iw_</th>
      <th>ix_</th>
      <th>ja_</th>
      <th>sx_</th>
      <th>sx_2</th>
      <th>se_</th>
      <th>sm_</th>
      <th>sn_</th>
      <th>so_</th>
      <th>cobirth</th>
      <th>empstat</th>
      <th>height</th>
      <th>weight</th>
      <th>heightm</th>
      <th>bmi</th>
      <th>bmirange</th>
      <th>hbloodpr</th>
      <th>k10sc</th>
      <th>k10scr</th>
      <th>asthmapa</th>
      <th>asthmacu</th>
      <th>diabtype</th>
      <th>urbrur</th>
      <th>incgrp</th>
      <th>smoker</th>
      <th>dwell</th>
      <th>phi</th>
      <th>educ</th>
      <th>occ</th>
      <th>lang</th>
      <th>se_3</th>
      <th>educ2</th>
      <th>smoker2</th>
      <th>incgrp2</th>
      <th>highbp</th>
      <th>hhsize</th>
      <th>walkonly</th>
      <th>vigonly</th>
      <th>abstaine</th>
      <th>diabetes</th>
      <th>overweig</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1278.0</td>
      <td>1.0</td>
      <td>3722</td>
      <td>1.0</td>
      <td>MANSFIELD</td>
      <td>52</td>
      <td>52.0</td>
      <td>NaN</td>
      <td>2.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>2.0</td>
      <td>NaN</td>
      <td>2.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>4.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>NaN</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>3.0</td>
      <td>777.0</td>
      <td>5.0</td>
      <td>0.0</td>
      <td>4.0</td>
      <td>11.0</td>
      <td>888.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>2.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>1.0</td>
      <td>NaN</td>
      <td>30.0</td>
      <td>2.0</td>
      <td>NaN</td>
      <td>14.0</td>
      <td>3.0</td>
      <td>2.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>3.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>2.0</td>
      <td>2.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>0.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>0.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>0.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>4.0</td>
      <td>2.0</td>
      <td>5.0</td>
      <td>NaN</td>
      <td>2.0</td>
      <td>3.0</td>
      <td>5.0</td>
      <td>2.0</td>
      <td>2.0</td>
      <td>2.0</td>
      <td>2.0</td>
      <td>2.0</td>
      <td>3.0</td>
      <td>2.0</td>
      <td>2.0</td>
      <td>2.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>1.0</td>
      <td>NaN</td>
      <td>2.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>2.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>2.0</td>
      <td>1.0</td>
      <td>2.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>2.0</td>
      <td>2.0</td>
      <td>2.0</td>
      <td>1.0</td>
      <td>2.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>2.0</td>
      <td>2.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>4.0</td>
      <td>1.0</td>
      <td>3.0</td>
      <td>4.0</td>
      <td>1.0</td>
      <td>4.0</td>
      <td>1.0</td>
      <td>2.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>2.0</td>
      <td>2.0</td>
      <td>4.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>7.0</td>
      <td>8.0</td>
      <td>CLEANING SCHOOL</td>
      <td>7.0</td>
      <td>1.0</td>
      <td>3.0</td>
      <td>2.0</td>
      <td>1.0</td>
      <td>90.952356</td>
      <td>90.952356</td>
      <td>4.0</td>
      <td>4.0</td>
      <td>4.0</td>
      <td>76.0</td>
      <td>1.0</td>
      <td>NaN</td>
      <td>149.860001</td>
      <td>NaN</td>
      <td>1.4986</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>NaN</td>
      <td>29.0</td>
      <td>3.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>9.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>2.0</td>
      <td>2.0</td>
      <td>3.0</td>
      <td>2.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>3.0</td>
      <td>4.0</td>
      <td>NaN</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>2.0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2574.0</td>
      <td>1.0</td>
      <td>3630</td>
      <td>1.0</td>
      <td>SHEPPARTON</td>
      <td>21</td>
      <td>21.0</td>
      <td>NaN</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>2.0</td>
      <td>3.0</td>
      <td>2.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>2.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>2.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>NaN</td>
      <td>1.0</td>
      <td>2.0</td>
      <td>6.0</td>
      <td>2.0</td>
      <td>1.0</td>
      <td>2.0</td>
      <td>0.0</td>
      <td>5.0</td>
      <td>11.0</td>
      <td>68.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>1.0</td>
      <td>8.0</td>
      <td>4.0</td>
      <td>7.0</td>
      <td>5.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>3.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>3.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>2.0</td>
      <td>2.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>105.0</td>
      <td>48.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>7.0</td>
      <td>10.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>4.0</td>
      <td>3.0</td>
      <td>4.0</td>
      <td>5.0</td>
      <td>5.0</td>
      <td>NaN</td>
      <td>4.0</td>
      <td>4.0</td>
      <td>5.0</td>
      <td>5.0</td>
      <td>2.0</td>
      <td>2.0</td>
      <td>2.0</td>
      <td>2.0</td>
      <td>NaN</td>
      <td>2.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>2.0</td>
      <td>2.0</td>
      <td>2.0</td>
      <td>2.0</td>
      <td>3.0</td>
      <td>2.0</td>
      <td>3.0</td>
      <td>4.0</td>
      <td>3.0</td>
      <td>4.0</td>
      <td>4.0</td>
      <td>4.0</td>
      <td>4.0</td>
      <td>4.0</td>
      <td>4.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>3.0</td>
      <td>4.0</td>
      <td>1.0</td>
      <td>2.0</td>
      <td>2.0</td>
      <td>2.0</td>
      <td>2.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>2.0</td>
      <td>2.0</td>
      <td>2.0</td>
      <td>2.0</td>
      <td>4.0</td>
      <td>3.0</td>
      <td>4.0</td>
      <td>3.0</td>
      <td>3.0</td>
      <td>3.0</td>
      <td>3.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>3.0</td>
      <td>0.0</td>
      <td>2.0</td>
      <td>4.0</td>
      <td>6.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>4.0</td>
      <td>2.0</td>
      <td>WORKING IN PUBS</td>
      <td>5.0</td>
      <td>3.0</td>
      <td>2.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>474.904761</td>
      <td>474.904761</td>
      <td>1.0</td>
      <td>4.0</td>
      <td>4.0</td>
      <td>19.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>180.339996</td>
      <td>68.0</td>
      <td>1.8034</td>
      <td>20.908594</td>
      <td>2.0</td>
      <td>NaN</td>
      <td>15.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>9.0</td>
      <td>1.0</td>
      <td>2.0</td>
      <td>3.0</td>
      <td>2.0</td>
      <td>1.0</td>
      <td>2.0</td>
      <td>2.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>2.0</td>
      <td>1.0</td>
      <td>3.0</td>
      <td>NaN</td>
      <td>3.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3864.0</td>
      <td>1.0</td>
      <td>3824</td>
      <td>1.0</td>
      <td>TRAFALGAR</td>
      <td>78</td>
      <td>78.0</td>
      <td>NaN</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>2.0</td>
      <td>1.0</td>
      <td>2.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>2.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>2.0</td>
      <td>4.0</td>
      <td>2.0</td>
      <td>2.0</td>
      <td>2.0</td>
      <td>2.0</td>
      <td>1.0</td>
      <td>3.0</td>
      <td>1.0</td>
      <td>4.0</td>
      <td>2.0</td>
      <td>3.0</td>
      <td>0.0</td>
      <td>6.0</td>
      <td>0.0</td>
      <td>85.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>1.0</td>
      <td>4.0</td>
      <td>6.0</td>
      <td>7.0</td>
      <td>3.0</td>
      <td>1.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>65.0</td>
      <td>17.0</td>
      <td>2.0</td>
      <td>2.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>3.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>2.0</td>
      <td>2.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>2.0</td>
      <td>3.0</td>
      <td>30.0</td>
      <td>0.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>0.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>1.0</td>
      <td>5.0</td>
      <td>5.0</td>
      <td>NaN</td>
      <td>5.0</td>
      <td>5.0</td>
      <td>NaN</td>
      <td>5.0</td>
      <td>5.0</td>
      <td>5.0</td>
      <td>5.0</td>
      <td>2.0</td>
      <td>2.0</td>
      <td>2.0</td>
      <td>2.0</td>
      <td>NaN</td>
      <td>2.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>4.0</td>
      <td>1.0</td>
      <td>2.0</td>
      <td>3.0</td>
      <td>3.0</td>
      <td>3.0</td>
      <td>NaN</td>
      <td>4.0</td>
      <td>1.0</td>
      <td>3.0</td>
      <td>2.0</td>
      <td>2.0</td>
      <td>3.0</td>
      <td>3.0</td>
      <td>3.0</td>
      <td>1.0</td>
      <td>2.0</td>
      <td>NaN</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>2.0</td>
      <td>2.0</td>
      <td>2.0</td>
      <td>2.0</td>
      <td>2.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>2.0</td>
      <td>2.0</td>
      <td>4.0</td>
      <td>1.0</td>
      <td>4.0</td>
      <td>1.0</td>
      <td>3.0</td>
      <td>4.0</td>
      <td>1.0</td>
      <td>3.0</td>
      <td>1.0</td>
      <td>2.0</td>
      <td>2.0</td>
      <td>0.0</td>
      <td>NaN</td>
      <td>2.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>4.0</td>
      <td>7.0</td>
      <td>FARM LABORER/CONSTRUCTION WORK</td>
      <td>7.0</td>
      <td>2.0</td>
      <td>1.0</td>
      <td>2.0</td>
      <td>1.0</td>
      <td>258.53585</td>
      <td>258.535850</td>
      <td>6.0</td>
      <td>5.0</td>
      <td>5.0</td>
      <td>30.0</td>
      <td>1.0</td>
      <td>3.0</td>
      <td>182.880005</td>
      <td>85.0</td>
      <td>1.8288</td>
      <td>25.414785</td>
      <td>3.0</td>
      <td>NaN</td>
      <td>8.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>9.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>2.0</td>
      <td>1.0</td>
      <td>2.0</td>
      <td>2.0</td>
      <td>2.0</td>
      <td>0.0</td>
      <td>2.0</td>
      <td>2.0</td>
      <td>2.0</td>
      <td>4.0</td>
      <td>NaN</td>
      <td>2.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>5143.0</td>
      <td>1.0</td>
      <td>3040</td>
      <td>1.0</td>
      <td>ESSENDON</td>
      <td>83</td>
      <td>83.0</td>
      <td>NaN</td>
      <td>2.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>2.0</td>
      <td>3.0</td>
      <td>2.0</td>
      <td>NaN</td>
      <td>1.0</td>
      <td>2.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>1.0</td>
      <td>4.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>2.0</td>
      <td>1.0</td>
      <td>4.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>2.0</td>
      <td>3.0</td>
      <td>0.0</td>
      <td>4.0</td>
      <td>9.0</td>
      <td>888.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>2.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>5.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>1.0</td>
      <td>2.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>3.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>2.0</td>
      <td>2.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>14.0</td>
      <td>8.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>0.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>1.0</td>
      <td>5.0</td>
      <td>5.0</td>
      <td>NaN</td>
      <td>5.0</td>
      <td>4.0</td>
      <td>5.0</td>
      <td>4.0</td>
      <td>5.0</td>
      <td>5.0</td>
      <td>5.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>2.0</td>
      <td>2.0</td>
      <td>2.0</td>
      <td>2.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>2.0</td>
      <td>3.0</td>
      <td>3.0</td>
      <td>4.0</td>
      <td>4.0</td>
      <td>3.0</td>
      <td>NaN</td>
      <td>2.0</td>
      <td>1.0</td>
      <td>4.0</td>
      <td>4.0</td>
      <td>4.0</td>
      <td>4.0</td>
      <td>4.0</td>
      <td>4.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>NaN</td>
      <td>1.0</td>
      <td>4.0</td>
      <td>2.0</td>
      <td>1.0</td>
      <td>2.0</td>
      <td>2.0</td>
      <td>2.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>NaN</td>
      <td>3.0</td>
      <td>2.0</td>
      <td>2.0</td>
      <td>1.0</td>
      <td>4.0</td>
      <td>4.0</td>
      <td>4.0</td>
      <td>4.0</td>
      <td>4.0</td>
      <td>1.0</td>
      <td>3.0</td>
      <td>1.0</td>
      <td>2.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>NaN</td>
      <td>4.0</td>
      <td>3.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>45.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>4.0</td>
      <td>7.0</td>
      <td>HOME DUTIES</td>
      <td>8.0</td>
      <td>7.0</td>
      <td>1.0</td>
      <td>2.0</td>
      <td>2.0</td>
      <td>496.553191</td>
      <td>496.553191</td>
      <td>6.0</td>
      <td>6.0</td>
      <td>6.0</td>
      <td>90.0</td>
      <td>1.0</td>
      <td>3.0</td>
      <td>144.779999</td>
      <td>NaN</td>
      <td>1.4478</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>NaN</td>
      <td>11.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>9.0</td>
      <td>2.0</td>
      <td>5.0</td>
      <td>3.0</td>
      <td>1.0</td>
      <td>2.0</td>
      <td>2.0</td>
      <td>3.0</td>
      <td>0.0</td>
      <td>2.0</td>
      <td>2.0</td>
      <td>1.0</td>
      <td>5.0</td>
      <td>NaN</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>2.0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>1152.0</td>
      <td>1.0</td>
      <td>3067</td>
      <td>1.0</td>
      <td>ABBOTSFORD</td>
      <td>29</td>
      <td>29.0</td>
      <td>NaN</td>
      <td>2.0</td>
      <td>1.0</td>
      <td>2.0</td>
      <td>2.0</td>
      <td>2.0</td>
      <td>2.0</td>
      <td>NaN</td>
      <td>1.0</td>
      <td>2.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>1.0</td>
      <td>2.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>NaN</td>
      <td>2.0</td>
      <td>2.0</td>
      <td>3.0</td>
      <td>5.0</td>
      <td>0.0</td>
      <td>5.0</td>
      <td>0.0</td>
      <td>5.0</td>
      <td>7.0</td>
      <td>60.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>1.0</td>
      <td>3.0</td>
      <td>5.0</td>
      <td>4.0</td>
      <td>3.0</td>
      <td>1.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>29.0</td>
      <td>15.0</td>
      <td>3.0</td>
      <td>3.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>3.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>2.0</td>
      <td>2.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>2.0</td>
      <td>NaN</td>
      <td>0.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>0.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>0.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>3.0</td>
      <td>5.0</td>
      <td>4.0</td>
      <td>5.0</td>
      <td>5.0</td>
      <td>4.0</td>
      <td>5.0</td>
      <td>5.0</td>
      <td>5.0</td>
      <td>5.0</td>
      <td>5.0</td>
      <td>2.0</td>
      <td>2.0</td>
      <td>2.0</td>
      <td>2.0</td>
      <td>2.0</td>
      <td>2.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>3.0</td>
      <td>3.0</td>
      <td>3.0</td>
      <td>4.0</td>
      <td>4.0</td>
      <td>3.0</td>
      <td>NaN</td>
      <td>4.0</td>
      <td>2.0</td>
      <td>4.0</td>
      <td>4.0</td>
      <td>4.0</td>
      <td>4.0</td>
      <td>4.0</td>
      <td>4.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>2.0</td>
      <td>2.0</td>
      <td>2.0</td>
      <td>2.0</td>
      <td>2.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>2.0</td>
      <td>2.0</td>
      <td>2.0</td>
      <td>4.0</td>
      <td>4.0</td>
      <td>4.0</td>
      <td>4.0</td>
      <td>2.0</td>
      <td>2.0</td>
      <td>2.0</td>
      <td>1.0</td>
      <td>2.0</td>
      <td>3.0</td>
      <td>0.0</td>
      <td>2.0</td>
      <td>2.0</td>
      <td>6.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>7.0</td>
      <td>2.0</td>
      <td>SALES REP</td>
      <td>5.0</td>
      <td>6.0</td>
      <td>3.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>1011.228897</td>
      <td>1011.228897</td>
      <td>2.0</td>
      <td>7.0</td>
      <td>7.0</td>
      <td>92.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>170.179993</td>
      <td>60.0</td>
      <td>1.7018</td>
      <td>20.717354</td>
      <td>2.0</td>
      <td>NaN</td>
      <td>12.0</td>
      <td>1.0</td>
      <td>NaN</td>
      <td>0.0</td>
      <td>9.0</td>
      <td>2.0</td>
      <td>4.0</td>
      <td>2.0</td>
      <td>2.0</td>
      <td>1.0</td>
      <td>3.0</td>
      <td>2.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>2.0</td>
      <td>1.0</td>
      <td>NaN</td>
      <td>3.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
  </tbody>
</table>
</div>
<br />

# Data Exploration
<div style="text-align: justify"> There are 237 features for each observation, and we are not going to use all of them. Therefore, I go through each of them to select some features. The following figure shows all selected features with their descriptions.</div>

<br />
![alt text](https://learn2gether.github.io/images/posts/bmi/selectedFeatures.png "Selected Features")
<br />

# Handling Missing Values


```python
selectedDf = df[['ik_','bs_2','sex','dr_','di_','cx_','in_','io_','ix_'
   ,'se_','height','weight','bmi','bmirange','urbrur','incgrp','smoker','dwell','educ']]
msno.bar(selectedDf)
```
<br />

![alt text](https://learn2gether.github.io/images/posts/bmi/missing_values.png "Missing Values")
<br />

<div style="text-align: justify"> There are 66 interviewees who do not provide their statistics for height, and 195 interviewees who do not provide their weights. By further investigation, we can find that once interviewees do not provide either their height or weight, the value of bmi will be filled by zero. Then, we cannot categorize their health level. Therefore, we should remove these samples.</div>
<br />

```python
selectedDf = selectedDf[selectedDf['bmi']!=0]
```

<br />

# Handling Outliers

<div style="text-align: justify"> An outlier refers to an instance has a huge difference compared to the majority of samples, which can be considered as an anomaly. Outliers also can be caused by manual input error. The most common approach is to remove them since they are not representative. The following figure shows that there are 33 outliers in the feature of the age, and its value is 999. It is impossible that a person has an age of 999, which should be caused by manual input error. Therefore, we need to remove this instance. </div>

```python
plt.figure(figsize=(16,2))
sns.boxplot(selectedDf['bs_2'],orient='h')
```
<br />

![alt text](https://learn2gether.github.io/images/posts/bmi/outliers.png "Outliers")
<br />

<div style="text-align: justify"> We have 27 observations with a BMI of either under 15 or over 50 (the lowest observed BMI is 10.96, and the highest one is 64.58). While these values may truly reflect the variation in our sample, they could be outliers due to measurement errors. </div>

```python
plt.figure(figsize=(16,2))
sns.boxplot(selectedDf['bmi'],orient='h')
```
<br />
![alt text](https://learn2gether.github.io/images/posts/bmi/outliers_bmi.png "Outliers BMI")
<br />

```python
cleanDf = selectedDf[~((selectedDf['bmi']>50)|(selectedDf['bmi']<15))]
```
<br />

# Exploratory Data Analysis

<div style="text-align: justify"> The following analysis is based on feature-selected data. The following figure reflects the distribution of class variable. By observing the whole sample population, we can see that the majority of people has a health condition considered as normal. There are 32% of interviewees are considered as overweight, and 16% of interviewees are considered as obese. There are only 3% of people considered as underweight. There are 51% of interviewees who may not be healthy, which may be caused by their lifestyles and eating habits. </div>
<br />

```python
plt.figure(figsize=(9,6))
total = float(len(cleanDf)) # one person per row 
#ax = sns.barplot(x="class", hue="who", data=titanic)
ax = sns.countplot(x="Health", data=cleanDf) # for Seaborn version 0.7 and more
for p in ax.patches:
    height = p.get_height()
    ax.text(p.get_x()+p.get_width()/2.,
            height + 3,
            '{:1.2f}'.format(height/total),
            ha="center") 
plt.show()
```
<br />

![alt text](https://learn2gether.github.io/images/posts/bmi/bmi_groups.png "BMI Groups")
<br />


## Height Weight and BMI

<div style="text-align: justify"> We cluster each observation based on weight and height, and it seems to reflect that the health condition has a closed correlation with the weight. </div>
<br />

```python
def healthy(x):
    if(x<18.5):
        return "Underweight"
    elif(x<25):
        return "Normal"
    elif(x<30):
        return "Overweight"
    else:
        return "Obese"
cleanDf["Health"]=cleanDf['bmi'].apply(healthy)
```

```python
sns.lmplot(data=cleanDf,x="weight",y="height",fit_reg=False,hue="Health",aspect=2)
```
<br />
![alt text](https://learn2gether.github.io/images/posts/bmi/cluster.png "clusters BMI")
<br />

```python
fig=plt.subplots(figsize=(10,10))
sns.violinplot(data=cleanDf,y="bmi",x="Health",hue="sex",color="Red")
plt.show()
```
<br />
![alt text](https://learn2gether.github.io/images/posts/bmi/bmi_health_condition.png "BMI vs. Health Condition")
<br />

<div style="text-align: justify"> we can see healthy people tend to have higher BMI. For example, healthy people should be categorized as Normal or underweight, and most of them gather in the top half part. </div>

## Gender

<div style="text-align: justify"> By comparing the health status of both male and female, there are more males who are considered as unhealthy (overweight and obese), which may reflect that females pay more attention on their health. The following figure also shows that there are more female observations considered as underweight. </div>
<br />

```python
fig = plt.figure(figsize=(20,8))
plt.title("Female vs Male comparison",loc='center',weight=10,size=15)
plt.xticks([]) # to disable xticks
plt.yticks([]) # to disable yticks

# first pie-plot
ax1 = fig.add_subplot(121)
ax1.axis('equal')
explode = (0.02,0.02,0.02,0.02)

wedges, texts, autotexts =  ax1.pie(cleanDf[cleanDf['sex']==2]['Health'].value_counts(),
                                    radius=0.8,
                                    explode=explode,
                                    labels=['Normal','Overweight','Obese','Underweight'],
                                    autopct="%1.1f%%",
                                    pctdistance=0.45,
                                    textprops=dict(color='k'),
                                    wedgeprops = { 'linewidth' : 3, 'edgecolor' : 'w' }
                                    )
plt.setp(autotexts,size=17)
plt.setp(texts,size=15)
my_circle = plt.Circle((0,0),0.5,color='white')
p = plt.gcf() # get current figure reference
p.gca().add_artist(my_circle) # get current axes
ax1.text(0,0,'Female',size=20,color='#1fa774',horizontalalignment='center',weight='bold')

# Second pie-plot
ax2 = fig.add_subplot(122)
ax2.axis('equal')
explode = (0.02,0.02,0.02,0.02)
wedges2, texts2, autotexts2 =  ax2.pie(cleanDf[cleanDf['sex']==1]['Health'].value_counts(),
                                    radius=0.8,
                                    explode=explode,
                                    labels=['Normal','Overweight','Obese','Underweight'],
                                    autopct="%1.1f%%",
                                    pctdistance=0.45,
                                    textprops=dict(color='k'),
                                    wedgeprops = { 'linewidth' : 3, 'edgecolor' : 'w' }
                                    )
plt.setp(autotexts2,size=17)
plt.setp(texts2,size=15)
my_circle = plt.Circle((0,0),0.5,color='white')
p = plt.gcf() # get current figure reference
p.gca().add_artist(my_circle) # get current axes
ax2.text(0,0,'Male',size=20,color='#1fa774',horizontalalignment='center',weight='bold')
```

<br />
![alt text](https://learn2gether.github.io/images/posts/bmi/gender.png "Gender")
<br />

## Geographic Analysis

<div style="text-align: justify"> According to the following analysis, we can visualize the number of obese people in each suburb. I scale
up based on the count since the maximum number is 27. The reason is that our sample population is not sufficient. The geographic visualization works well on a large population, which could help us find whether some suburbs have a relatively large proportion of obese people. </div>
<br />

![alt text](https://learn2gether.github.io/images/posts/bmi/geoMaps.png "Geo Maps")
<br />

# Modelling

<div style="text-align: justify"> Ordinal regression is also known as ordered, and it is used to predict the dependent variable with ‘ordered’ multiple categories and independent variables. The most common ordered outcome dependent variable is rating systems, and there are normally several evaluations to rate such as poor, fair, good and excellent. The analysis of BMI is quite similar as the rating systems, and it is the heath rating systems. Observations can be categorised as underweight, normal, overweight and obese. We can see that our dependent variables are categorical variables. These variables are not numbers, but they can be ordered from high to low using certain criteria. In order to fit them into the model by the software, we coded them as 1, 2, 3 and 4 representing underweight, normal, overweight and obese respectively. </div>
<br />

## Ordered Logit Model

<br />
![alt text](https://learn2gether.github.io/images/posts/bmi/ordered_logit_model.png "ordered logit model")
<br />

<div style="text-align: justify"> According to the result, we can see that as people are getting older, they are less likely to be in the higher categories of BMI. In addition, if people have diseases such as diabetes and high blood pressure, they are more likely to be in the higher categories of BMI. As our analysis above, it also reflects these results. It appears that there are many predictors which are statistically significant (pvalue is less than 0.05) including age, gender, diseases, marital status, smoking status and so on. Therefore, these two variables are major determinants of obesity. </div>
<br />
<div style="text-align: justify"> In the set of coefficients, there are three intercepts (threshold) parameters. If these parameters are significantly different from each other, these four categories should not be combined into one. Sometimes, we may have many categories, it is hard for the model to distinguish between these categories and find any significant results or independent variables. In this case, we better combine some categories. For example, the opinion survey, the strongly agree and agree may not help observations make a distinct choice. In addition, if one category only has a few observations, it is hard for the model to give a meaningful result.</div>
<br />

## Ordered Probit Model
<div style="text-align: justify"> By comparing marginal effects of both models, the ordered logit and probit models produce almost identical results. </div>

<br />

![alt text](https://learn2gether.github.io/images/posts/bmi/probit_logit_model.png "ordered probit model")

<br />

## Multinomial Logit Model
<div style="text-align: justify"> Multinomial logit Regression is a type of linear regression for the nomial dependent variable with more than two levels. It is used to describe data and to explain the relationship between one dependent nominal variable and one or more independent variables. The following figure presents the coefficients of MLR. Underweight is considered as our baseline level. We can see that the first row is being compared to BMI = “Normal” to our baseline level, the second row to BMI = “Overweight” to our baseline level, and the third row to BMI = “Obese” to our baseline level. As for the interpretation of age, one unit increase in age increases the log odds of being in the category of Normal vs. the category of Underweight by 0.004. As for the gender, the log odds of being in the category of Normal than in the category of Underweight will increase by 0.816 for male. </div>
<br />
![alt text](https://learn2gether.github.io/images/posts/bmi/MLR_coef.png "MLR Coefficient")
<br />

<div style="text-align: justify"> From the following figure, we can see that gender (male) is significant for each level of the dependent variables. </div>
<br />
![alt text](https://learn2gether.github.io/images/posts/bmi/MLR_significance.png "MLR Significance")
<br />

# Model Selection

## Likelihood Ratio Test

<div style="text-align: justify"> By comparing the value of log likelihood, the closer the log likelihood gets to zero, the better the model fits since the value of log likelihood is always negative. According to the results of two models, ordered probit model is better choice. However, both models have an almost identical result.</div>
<br />

## Akaike Information Criterion
<div style="text-align: justify"> AIC is another benchmark for the model selection. Smaller AIC is better by the criterion. Normally, we deal with models which have much larger differences at BIC scores, and one model with smallest AIC would be preferred model. However, in our case, these three models are generating a nearly identical job, and the multinomial logit model has a relatively low AIC, which makes it a better choice. Therefore, AIC may not a suitable indicator. </div>
<br >

# Limitation
<div style="text-align: justify"> There are many limitations to the collected data since this information is generated from the survey. It means that people tend to provide information not revealing their personal details resulting in inaccuracy, which may mislead the model to identify real patterns. For example, people may not measure their height and weight frequently, and they usually provide a rough number. Also, some people care about their appearance, so they may provide fault information about their demographics. In addition, if the information is about their health condition such as diseases, they normally refuse to provide. Therefore, there are a huge amount of missing values for some features which could be major determinants of obesity. Furthermore, survey data is collected in a specific period of time, some interviewees’ behaviors may change recently, and these behaviors could lead to a life-changing transformation to their lifestyles or eating habits. For example, smoking and drinking could have an impact on diabetes and high blood pressure. Some interviewees may quit drinking and smoking for only a few weeks, which may not relieve their symptoms. Therefore, they may provide distinct information in another survey.</div>
<br />

# Suggestions On Monetary Implications
<div style="text-align: justify"> According to the analysis above, some features are pointed out as potential determinants of obesity by all models such as diabetes, high blood pressure, smoking status and education level. At first, the government should strengthen education about the impact of obesity and its causes, which can avoid forming bad habits in the first place. For example, schools and local communities should provide extra lessons about a healthy lifestyle. These actions could help the government balance the budge in the long run. In addition, smoking and excessive drinking can cause many diseases, which directly increases the government expenditure on medical benefits. Thus, the government could raise more taxes on these products, which can prevent overuse while increasing financial revenues. </div>
<br />




